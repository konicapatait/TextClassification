# -*- coding: utf-8 -*-
"""MSDSTextClassification_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YP9AW9Ei3nzh3iKr4kWNYVVv4snuGBjj

# Text Classification: Building a model with K-Train

## About this Project

The goal of building an inference model for predicting whether or not a document is about "healthy living." We will use the K-Train library to do this. we will explore ways to tweak and improve the model with hyperparameters

## Imports

We're going to be using Google's Tensorflow package:
https://www.tensorflow.org/tutorials

We're using an API wrapper for Tensorflow called **ktrain**. It abstracts the whole deep learning process into a workflow:
https://github.com/amaiya/ktrain
"""

import os

!pip install -U tf_keras # Keras 2
os.environ["TF_USE_LEGACY_KERAS"] = "1"
print("OS Environment Set ")
!pip install tensorflow==2.15.1
import tensorflow as tf
print("tensorflow version", tf.__version__)


try:
  print("Importing Ktrain")
  import ktrain
except:
  print("------In Except while installing Ktrain------")
  !pip install ktrain
  print("Now Killing Process")
  #os.kill(os.getpid(), 9)
  print("Process Killed")
import ktrain
import pandas as pd
import numpy as np

from tf_keras.optimizers import Adam

print("All files loaded successfully...!!!")

"""## Mount Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""## Set your google colab runtime to use GPU, a must for deep learning!

Runtime > Change Runtime Type > GPU

The following code snippet will show you GPU information for your runtime.
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

"""## Load the data

The data file should be in your Google Drive from Lab 1.
"""

reviews = pd.read_json("/content/drive/MyDrive/MSDS/ColabNotebooks/marketing_text_analytics/master_files/text_classification/news_category_trainingdata.json")

"""## Inspect the data"""

reviews.head()

"""## Prepare the data

Most machine learning tools in Python accept one field/column/string. So we have to merge our two text column. Let's separate it with a space.
"""

reviews['combined_text'] = reviews['headline'] + ' ' + reviews['short_description']

"""The first thing we need to do is prepare the data. Specifically, we have a categorical column that we want to turn into a "is this article healthy living?" column. That is, when an article is about healthy living, it should have a 1, when it's anything else, it should be a 0."""

reviews[reviews['category'].str.contains("HEALTHY LIVING")]

reviews['healthy'] = np.where((reviews['category'] == 'HEALTHY LIVING'), 1, 0)

reviews[reviews['healthy'] == 1].count()

reviews['healthy'].describe()

"""## Balance the data

To create a balanced data set that includes all of the health living articles, set sample_amount to the total number of those articles.

In Lab 1, you balanced the data for the full set of healthy living articles. In the interest of getting through Lab 2 more quickly (in terms of training time for the model), we will use a smaller sample, of just 1000 articles per class. After completing the lab, consider increasing the sample size to see if you can get improvements on the model performance. Of course, be prepared for longer training times when you do that.
"""

# We have replaced the sample count with a smaller number in order to expedite
# the completion of the lab. We want to use the full balanced document set which
# is determined by this commented line:
#sample_amount =  len(reviews[reviews["healthy"] == 1]) # the total number of healthy living articles

sample_amount =  len(reviews[reviews["healthy"] == 1])
print("Sample Amount: ", sample_amount )

healthy = reviews[reviews['healthy'] == 1].sample(n=sample_amount)
not_healthy = reviews[reviews['healthy'] == 0].sample(n=sample_amount)

review_sample = pd.concat([healthy,not_healthy])

review_sample.describe()

"""# On to Lab 2: Test, Tune and Save Models

Here, you will tune and train a predictor model for classifying healthy-living articles. After completing this lab, complete the Lab Quiz by entering your precision and recall values from the validation report for both the negative and positive classes.
"""

target_names = ['NOT HEALTHY LIVING','HEALTHY LIVING']

"""---

### Experimenting with different transformers

For purposes of this lab, we are using the **distilbert-base-uncased** transformer model. Other models you might try for your final project include:

 * roberta-base
 * bert-base-uncased
 * distilroberta-base

See all the models here: https://huggingface.co/transformers/pretrained_models.html

Some work, some dont, try at your own risk.

---
"""

train, val, preprocess = ktrain.text.texts_from_df(
    review_sample,
    "combined_text",
    label_columns=["healthy"],
    val_df=None,
    max_features=30000,
    maxlen=512,
    val_pct=0.1,
    ngram_range=1,
    preprocess_mode="distilbert", # Try roberta, bert
    verbose=1
)

optimizer = Adam(learning_rate=0.001)
print('optimizer created successfully..!')


model = preprocess.get_classifier()
print('Model created successfully..!')

learner = ktrain.get_learner(model, train_data=train, val_data=val, batch_size=16)
print('Learner created successfully..!')

learner.lr_find(max_epochs=6)

learner.lr_plot()

"""Now, use the tuned learner to train the best model.

Here, we define a limit of 10 epochs, but in reality, this should stop much sooner due to early stopping.
"""

history=learner.autofit(
    1e-4,
    checkpoint_folder='checkpoint',
    epochs=10,
    early_stopping=True
)

"""Get the predictor"""

predictor = ktrain.get_predictor(learner.model, preproc=preprocess)

"""This code to save the predictor and reload it later. Note, the saved models can be quite large."""

predictor.save("drive/MyDrive/MSDSTextClassification_Lab2.healthy_living")

validation = learner.validate(val_data=val, print_report=True)

"""# Inspecting the drivers of prediction

No matter what the supervised machine learning model, you always want to peak under the hood to see what features are driving prediction. That is, what words sway the outcome of the prediction. It's harder to inspect a neural network. Because all of the layers of a neural network aren't really interpretable to the human eye.

Currently, the best practice I've found is a little tool Explainable AI:
https://alvinntnu.github.io/python-notes/nlp/ktrain-tutorial-explaining-predictions.html
"""

!pip3 install -q git+https://github.com/amaiya/eli5@tfkeras_0_10_1

"""Let's go ahead and make a little set of test documents to check out"""

test_docs = [
'Stress May Be Your Heart’s Worst Enemy Psychological stress activates the fear center in the brain, setting into motion a cascade of reactions that can lead to heart attacks and strokes.',
'Exercising to Slim Down? Try Getting Bigger. It’s high time for women to reclaim the real strength behind exercise.',
'What Are Your Food Resolutions for the New Year? Join us for the Eat Well Challenge starting in January.',
'Why We All Need to Have More Fun. Prioritizing fun may feel impossible right now. But this four-step plan will help you rediscover how to feel more alive.',
'Cuomo Will Not Be Prosecuted in Groping Case, Albany D.A. Says. The district attorney described the woman who said former Gov. Andrew Cuomo had groped her as “credible,” but added that proving her allegation would be difficult.',
'A Film Captures Jewish Life in a Polish Town Before the Nazis Arrived. A documentary based on a home movie shot by an American in 1938 provides a look at the vibrancy of a Jewish community in Europe just before the Holocaust.'
             ]

loaded_predictor = ktrain.load_predictor("drive/MyDrive/MSDSTextClassification_Lab2.healthy_living")

for i, text in enumerate(test_docs):
  probs = loaded_predictor.predict(text, return_proba=True)
  print("---------------------------")
  print('The probability this is healthy is %s' % probs[1])
  print(text)

"""*These* are pretty obvious examples, but it works exactly as expected!"""

prob = loaded_predictor.predict('Diversity is the key to a healthy society. Here is what we need to do to make america a more equitable place to live for all.', return_proba=True)
print("---------------------------")
print('The probability this is healthy is %s' % prob[1])

"""But you can see, this algorithm is far from perfect. Here you can see that it's probably got too high of an emphasis on the word "healthy."
"""